{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Neural Network and evaluating it's performance on the handwritten digits data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sravanthi/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.io import loadmat  \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder  \n",
    "from scipy.optimize import minimize\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "data = loadmat('ex3data1.mat')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We have imported the hand-written digits data set. This dataset consists of 5000 instances with each training example of 20X20 (Gray scale format). Hence, the input_size is 400 (20X20). We then split the data into training and testing test using train_test_split method. Please note that we use only the training set for our cross validation (This will be discussed further). Also, this dataset is taken from MNIST dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 400), (4000, 1), (1000, 400), (1000, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['X']  \n",
    "y = data['y']  \n",
    "\n",
    "\n",
    "X,X_test,y,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "kfl=KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "X.shape, y.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we used one-hot encoding as it turns a class label 'n' out of k classes into a vector of length k. We used Scikit's library for this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (4000, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)  \n",
    "y_onehot = encoder.fit_transform(y)  \n",
    "y_onehot.shape\n",
    "\n",
    "\n",
    "y_onehot_test=encoder.fit_transform(y_test)\n",
    "y_onehot_test.shape, y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8], dtype=uint8),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0], y_onehot[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sigmoid Function Implementation: This would bring the prediction values to range of 0 to 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):  \n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Forward Propagation Implementation: Forward Propagation function calculates the hypothesis for each training instance given the current parameters at each layer in the network. The hypothesis vector 'h', which contains the prediction probabilities for each class, should match our one-hot encoding for y. The cost function then calculates the accuracy of the hypothesis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2):  \n",
    "    m = X.shape[0]\n",
    "\n",
    "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
    "    z2 = a1 * theta1.T\n",
    "    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n",
    "    z3 = a2 * theta2.T\n",
    "    h = sigmoid(z3)\n",
    "\n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cost Function Implementation: Implements the cost function for determining the error of the neural netwoek predictions. It is required for the competition of accuracy of predictions when we test the network and also for the back propagation algorithm. We need the initial set of parameters as well as input layer size, hidden layer size, no of labels and the learning rate.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(params, input_size, hidden_size, num_labels, X, y, learning_rate):  \n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "\n",
    "    \n",
    "    print (h.shape)\n",
    "    # compute the cost\n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
    "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
    "        J += np.sum(first_term - second_term)\n",
    "\n",
    "    J = J / m\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output layer will have 10 units corresponding to the one hot encoding for the class labels.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial setup\n",
    "\n",
    "input_size = 400 #20*20 grayscale image\n",
    "hidden_size = 25 # One hidden layer with 25 nodes or units\n",
    "num_labels = 10\n",
    "learning_rate = 1\n",
    "\n",
    "# randomly initialize a parameter array of the size of the full network's parameters\n",
    "params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
    "\n",
    "m = X.shape[0] \n",
    "print (m)\n",
    "X = np.matrix(X)  \n",
    "y = np.matrix(y)\n",
    "\n",
    "# unravel the parameter array into parameter matrices for each layer\n",
    "theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))  \n",
    "theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "theta1.shape, theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 401), (4000, 25), (4000, 26), (4000, 10), (4000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)  \n",
    "a1.shape, z2.shape, a2.shape, z3.shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.7259471099455803"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Next we have to add regularization to the cost function. Before that we have to compute the gradient of the sigmoid function which we implemented earlier. This is the derivative of the sigmoid.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):  \n",
    "    return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back Propagation Function Implementation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the first half of the function, we are calculating the error by running the data and the current parameters through the \"network\" (the forward-propagate function) and comparing the output to the true labels. The total error across the whole data set is represented as 'J' and this is the part we discussed earlier as the cost function. After this, we are adjusting the parameters to reduce the error the next time we run through the network. We did this by computing the contributions at each layer to the total error and adjusting appropriately by coming up with a \"gradient\" matrix (or, how much to change each parameter and in what direction). *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(params, input_size, hidden_size, num_labels, X, y, learning_rate):  \n",
    "    ##### this section is identical to the cost function logic we already saw #####\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "\n",
    "    # initializations\n",
    "    J = 0\n",
    "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
    "    delta2 = np.zeros(theta2.shape)  # (10, 26)\n",
    "\n",
    "    # compute the cost\n",
    "    for i in range(m):\n",
    "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
    "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
    "        J += np.sum(first_term - second_term)\n",
    "\n",
    "    J = J / m\n",
    "\n",
    "    # add the cost regularization term\n",
    "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
    "\n",
    "    ##### end of cost function logic, below is the new part #####\n",
    "\n",
    "    # perform backpropagation\n",
    "    for t in range(m):\n",
    "        a1t = a1[t,:]  # (1, 401)\n",
    "        z2t = z2[t,:]  # (1, 25)\n",
    "        a2t = a2[t,:]  # (1, 26)\n",
    "        ht = h[t,:]  # (1, 10)\n",
    "        yt = y[t,:]  # (1, 10)\n",
    "\n",
    "        d3t = ht - yt  # (1, 10)\n",
    "\n",
    "        z2t = np.insert(z2t, 0, values=np.ones(1))  # (1, 26)\n",
    "        d2t = np.multiply((theta2.T * d3t.T).T, sigmoid_gradient(z2t))  # (1, 26)\n",
    "\n",
    "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
    "        delta2 = delta2 + d3t.T * a2t\n",
    "\n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "\n",
    "    # add the gradient regularization term\n",
    "    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * learning_rate) / m\n",
    "    delta2[:,1:] = delta2[:,1:] + (theta2[:,1:] * learning_rate) / m\n",
    "\n",
    "    # unravel the gradient matrices into a single array\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.7326112431925793, (10285,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, grad = backprop(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)  \n",
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This can be treated as initializing the model. Minimize is an inbuilt scipy method. We train our neural network with the training dataset. Resulting fmin is considered as our model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.37287060496720426\n",
       "     jac: array([  3.88503121e-04,   2.12856004e-06,  -3.23313603e-06, ...,\n",
       "        -7.71775952e-04,  -1.06789053e-04,  -8.45805727e-05])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 250\n",
       "     nit: 18\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([-1.16207383,  0.00851424, -0.01293254, ...,  0.4938369 ,\n",
       "       -0.89590773, -1.28918626])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minimize the objective function\n",
    "\n",
    "#num_labels_2=4\n",
    "fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, learning_rate),  \n",
    "                method='TNC', jac=True, options={'maxiter': 250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, we obtain our predictions from forward_propagation. This stage is considered as prediction.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [ 4],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 3],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 4],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 8],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 4],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 4],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [ 8],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 8],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [ 6],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 8],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 4],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 3],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 6],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 3],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [ 3],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 9],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 3],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 9],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 3],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 2],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 8],\n",
       "       [10],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 4],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 9],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 6],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [10],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [ 3],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 5],\n",
       "       [ 7],\n",
       "       [ 2],\n",
       "       [ 6],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [ 9],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 6],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 3],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 8],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 5],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [ 4],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [ 9],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [ 5],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 5],\n",
       "       [10],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [ 1],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 8],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 7],\n",
       "       [ 3],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 4],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 9],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [ 2],\n",
       "       [ 7],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 3],\n",
       "       [10],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 6],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 5],\n",
       "       [10],\n",
       "       [ 9],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [ 4],\n",
       "       [ 6],\n",
       "       [ 5],\n",
       "       [ 3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.matrix(X_test)  \n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))  \n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "a1, z2, a2, z3, h = forward_propagate(X_test, theta1, theta2)  \n",
    "y_pred = np.array(np.argmax(h, axis=1) + 1)  \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our implementation without KFold 93.10000000000001%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y_test)]  \n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print ('Accuracy of our implementation without KFold ' +str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To obtain an optimal performance, we used kfold cross validation. We split our traning set into training and validation set. Number of folds used here is 5. The model is trained during all the 5 folds and the best model is returned, where it is tested with the actual testing dataset and the accuracy is computed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNeuralNetsWithKfold(X,y,kfl):\n",
    "    accuracyArr=[]\n",
    "    fminArr=[]\n",
    "    for train,test in kfl.split(X):\n",
    "        \n",
    "      #  print (X[train].shape,X[test].shape,y[train].shape,y[test].shape)\n",
    "        \n",
    "        X_1,X_test,y_1,y_test= X[train],X[test],y[train],y[test]\n",
    "       # print (X.shape, X_test.shape, y.shape, y_test.shape)\n",
    "        \n",
    "        #print (train, test)\n",
    "        y_onehot =  encoder.fit_transform(y_1) \n",
    "        fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size, num_labels, X_1, y_onehot, learning_rate),  \n",
    "                method='TNC', jac=True, options={'maxiter': 250})\n",
    "        \n",
    "        \n",
    "        fminArr.append(fmin)\n",
    "        X_test1 = np.matrix(X_test)  \n",
    "        theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))  \n",
    "        theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "        a1, z2, a2, z3, h = forward_propagate(X_test1, theta1, theta2)  \n",
    "        y_pred = np.array(np.argmax(h, axis=1) + 1)  \n",
    "        correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y_test)]  \n",
    "        accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "        accuracyArr.append(accuracy)\n",
    "        print ('Processing Fold '+str(len(accuracyArr)))\n",
    "    \n",
    "    bestfmin= fminArr[accuracyArr.index(np.amax(accuracyArr))]\n",
    "    \n",
    "    return bestfmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n",
      "Processing Fold 2\n",
      "Processing Fold 3\n",
      "Processing Fold 4\n"
     ]
    }
   ],
   "source": [
    "#kfl=KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "fmin=computeNeuralNetsWithKfold(X,y,kfl)\n",
    "#print (fmin)\n",
    "print ('Calculated Bestfmin (Best model)')\n",
    "\n",
    "#This is where the best model is tested with the actual testing dataset.\n",
    "\n",
    "X_t = np.matrix(X_test)  \n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))  \n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "a1, z2, a2, z3, h = forward_propagate(X_t, theta1, theta2)  \n",
    "y_pred = np.array(np.argmax(h, axis=1) + 1)  \n",
    "\n",
    "print ('Calculated Y_pred')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Kfold cross validation 92.7%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y_test)]  \n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print ('Accuracy with Kfold cross validation ' +str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
